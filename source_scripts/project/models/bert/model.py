# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb.

# %% auto 0
__all__ = ['Embedding', 'get_attn_pad_mask', 'ScaledDotProductAttention', 'MultiHeadAttention', 'PositionwiseFeedForward',
           'EncoderLayer', 'gelu', 'BERT']

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 2
import numpy as np

# Imports
import torch
import torch.nn as nn
import torch.nn.functional as F

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 4
class Embedding(nn.Module):
    def __init__(
        self,
        max_length,
        vocab_size,
        d_model,
    ):
        super(
            Embedding,
            self,
        ).__init__()
        # self.tok_embed.shape = (vocab_size, d_model)
        self.tok_embed = nn.Embedding(
            vocab_size,
            d_model,
        )  # token embedding
        self.pos_embed = nn.Embedding(
            max_length,
            d_model,
        )  # position embedding
        self.norm = nn.LayerNorm(d_model)  # layer normalization

    def forward(
        self,
        x,
    ):
        """
        x: (batch_size, seq_len)
        """
        seq_len = x.size(1)
        pos = torch.arange(
            seq_len,
            dtype=torch.long,
        ).cuda()  # 0, 1, 2,... , seq_len-1
        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (1, seq_len) -> (batch_size, seq_len)
        embedding = self.tok_embed(x) + self.pos_embed(pos)  # + self.seg_embed(seg) # (batch_size, seq_len, d_model)
        return self.norm(embedding)  # (batch_size, seq_len, d_model)

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 7
def get_attn_pad_mask(
    seq_q,
):
    """For masking out the padding part of key sequence.
    This ensures that the attention mechanism does not attend to padding tokens, which do not contain any useful information.
    """
    (
        batch_size,
        len_q,
    ) = seq_q.size()  # (batch_size, len_q)
    # eq(zero) is PAD token
    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # (batch_size, 1, len_q), True is masked
    return pad_attn_mask.expand(
        batch_size,
        len_q,
        len_q,
    )  # (batch_size, len_q, len_q)

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 12
class ScaledDotProductAttention(nn.Module):
    def __init__(
        self,
        d_k,
    ):
        super(
            ScaledDotProductAttention,
            self,
        ).__init__()
        self.d_k = d_k

    def forward(
        self,
        Q,
        K,
        V,
        attn_mask,
    ):
        """
        Q, K, V: (batch_size, seq_len, d_model)
        attn_mask: (batch_size, seq_len, seq_len)
        """
        scores = torch.matmul(
            Q,
            K.transpose(
                -1,
                -2,
            ),
        ) / np.sqrt(
            self.d_k
        )  # scores: (batch_size, seq_len, seq_len)
        scores.masked_fill_(
            attn_mask,
            float("-inf"),
        )  # Fills elements of self tensor with value where mask is True.
        attn = nn.Softmax(dim=-1)(scores)
        context = torch.matmul(
            attn,
            V,
        )  # (batch_size, seq_len, d_model)
        return (
            scores,
            context,
            attn,
        )

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 15
class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        d_k,
        d_model,
        num_heads,
    ):
        super(
            MultiHeadAttention,
            self,
        ).__init__()
        self.d_k = d_k
        self.num_heads = num_heads
        self.d_model = d_model
        self.M_Q = nn.Linear(
            d_model,
            d_k * num_heads,
        )
        self.M_K = nn.Linear(
            d_model,
            d_k * num_heads,
        )
        self.M_V = nn.Linear(
            d_model,
            d_k * num_heads,
        )
        self.scaled_dot_product_attention = ScaledDotProductAttention(d_k)
        self.output_linear = nn.Linear(
            num_heads * d_k,
            d_model,
        )
        self.norm = nn.LayerNorm(d_model)

    def forward(
        self,
        Q,
        K,
        V,
        attn_mask,
    ):
        """
        Q, K, V: (batch_size, seq_len, d_model)
        attn_mask: (batch_size, seq_len, seq_len)
        """
        (
            residual,
            batch_size,
        ) = (
            Q,
            Q.size(0),
        )
        q_s = self.M_Q(Q)  # q_s: (batch_size, seq_len, d_k * num_heads)
        q_s = q_s.view(
            batch_size,
            -1,
            self.num_heads,
            self.d_k,
        )  # q_s: (batch_size, seq_len, num_heads, d_k)
        q_s = q_s.transpose(
            1,
            2,
        )  # q_s: (batch_size, num_heads, seq_len, d_k)

        k_s = self.M_Q(K).view(
            batch_size,
            -1,
            self.num_heads,
            self.d_k,
        )  # k_s: (batch_size, seq_len, num_heads, d_k)
        k_s = k_s.transpose(
            1,
            2,
        )  # k_s: (batch_size, num_heads, seq_len, d_k)

        v_s = self.M_Q(V).view(
            batch_size,
            -1,
            self.num_heads,
            self.d_k,
        )  # v_s: (batch_size, seq_len, num_heads, d_k)
        v_s = v_s.transpose(
            1,
            2,
        )  # v_s: (batch_size, num_heads, seq_len, d_k)

        attn_mask = attn_mask.unsqueeze(1)  # attn_mask: (batch_size, 1, seq_len, seq_len)
        attn_mask = attn_mask.repeat(
            1,
            self.num_heads,
            1,
            1,
        )  # attn_mask: (batch_size, num_heads, seq_len, seq_len)

        (
            scores,
            context,
            attn,
        ) = self.scaled_dot_product_attention(
            q_s,
            k_s,
            v_s,
            attn_mask,
        )  # context: (batch_size, num_heads, seq_len, d_k)
        context = (
            context.transpose(
                1,
                2,
            )
            .contiguous()
            .view(
                batch_size,
                -1,
                self.num_heads * self.d_k,
            )
        )  # context: (batch_size, seq_len, num_heads * d_k)

        output = self.output_linear(context)  # output: (batch_size, seq_len, d_model)

        return (
            self.norm(output + residual),
            attn,
        )  # output: (batch_size, seq_len, d_model), attn: (batch_size, num_heads, seq_len, seq_len)


class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."

    def __init__(
        self,
        d_model,
        d_ff,
        dropout=0.1,
    ):
        super(
            PositionwiseFeedForward,
            self,
        ).__init__()
        self.w_1 = nn.Linear(
            d_model,
            d_ff,
        )
        self.w_2 = nn.Linear(
            d_ff,
            d_model,
        )
        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        x,
    ):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 16
class EncoderLayer(nn.Module):
    def __init__(
        self,
        d_model=1024,
        d_k=64,
        num_heads=12,
    ):
        super(
            EncoderLayer,
            self,
        ).__init__()
        self.enc_self_attn = MultiHeadAttention(
            d_k=d_k,
            d_model=d_model,
            num_heads=num_heads,
        )
        self.feed_forward = PositionwiseFeedForward(
            d_model,
            d_ff=d_model * 4,
        )

    def forward(
        self,
        enc_input,
        enc_self_attn_mask,
    ):
        (
            enc_output,
            attn,
        ) = self.enc_self_attn(
            Q=enc_input,
            K=enc_input,
            V=enc_input,
            attn_mask=enc_self_attn_mask,
        )  # enc_input to same Q,K,V
        enc_output = self.feed_forward(enc_output)
        return (
            enc_output,
            attn,
        )

# %% ../../../../notebooks/models/BERT/02_BERT-reimp.ipynb 18
import math


def gelu(
    x,
):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))


class BERT(nn.Module):
    def __init__(
        self,
        max_length,
        vocab_size,
        d_k=64,
        d_model=1024,
        num_heads=12,
        n_layers=6,
    ):
        super(
            BERT,
            self,
        ).__init__()
        self.embedding = Embedding(
            max_length,
            vocab_size,
            d_model,
        )
        self.layers = nn.ModuleList(
            [
                EncoderLayer(
                    d_model,
                    d_k,
                    num_heads,
                )
                for _ in range(n_layers)
            ]
        )
        self.fc = nn.Linear(
            d_model,
            d_model,
        )
        self.activ1 = nn.Tanh()
        self.linear = nn.Linear(
            d_model,
            d_model,
        )
        self.activ2 = gelu
        self.norm = nn.LayerNorm(d_model)

        embed_weight = self.embedding.tok_embed.weight

        (
            n_vocab,
            n_dim,
        ) = embed_weight.size()
        self.decoder = nn.Linear(
            n_dim,
            n_vocab,
            bias=False,
        )
        self.decoder.weight = embed_weight
        self.decoder.bias = nn.Parameter(torch.zeros(n_vocab))

    def forward(
        self,
        inputs,
        masked_pos,
    ):
        output = self.embedding(inputs)  # (batch_size, seq_len, d_model)
        enc_self_attn_mask = get_attn_pad_mask(inputs).cuda()  # (batch_size, seq_len, seq_len)
        for layer in self.layers:
            (
                output,
                enc_self_attn,
            ) = layer(
                output,
                enc_self_attn_mask,
            )  # output: (batch_size, seq_len, d_model)

        # The tensor `masked_pos`  indicates the positions of the masked tokens in the input sequence.
        # The tensor `output` is the output of the BERT model, which is a tensor of shape `(batch_size, seq_len, d_model)`.
        masked_pos = masked_pos[
            :,
            :,
            None,
        ].expand(
            -1,
            -1,
            output.size(-1),
        )  # (batch_size, seq_len, d_model)

        h_masked = torch.gather(
            output,
            1,
            masked_pos,
        )  # (batch_size, 1, d_model), gather the masked tokens
        h_masked = self.norm(self.activ2(self.linear(h_masked)))  # h_masked: (batch_size, 1, d_model) -> Linear(d_model, d_model) -> GELU -> LayerNorm -> (batch_size, 1, d_model)
        logist_lm = self.decoder(h_masked) + self.decoder.bias  # (batch_size, 1, n_vocab)
        return logist_lm
