{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bb3ff-f742-454d-8080-5e60d8175299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp project.models.bert.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6610b",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "![](/Users/mcaro/workbench-ai/dman-nebula-micromachines-build/notebooks/models/bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546b148-f35b-431f-a8a3-ef7810c1d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83e23d",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "The code below defines a PyTorch module called `Embedding` that represents an embedding layer. The module takes in four arguments: \n",
    "- `max_length`, which is the maximum length of the input sequence; \n",
    "- `n_segments`, which is the number of segments in the input sequence; \n",
    "- `vocab_size`, which is the size of the vocabulary; and \n",
    "- `d_model`, which is the size of the embedding vector.\n",
    "\n",
    "The `__init__` method initializes the embedding layer and a layer normalization module. The embedding layer is created using three `nn.Embedding` modules: one for the token embeddings, one for the positional embeddings, and one for the segment embeddings. The positional embeddings are generated using the `torch.arange` function to create a tensor of integers from 0 to `seq_len-1`, where `seq_len` is the length of the input sequence. The segment embeddings are created using the `n_segments` argument. The layer normalization module is created using the `nn.LayerNorm` module.\n",
    "\n",
    "The `forward` method takes in two input tensors: \n",
    "- `x`, which is the input sequence, and \n",
    "- `seg`, which is the segment IDs of the input sequence. \n",
    " \n",
    "The method first calculates the sequence length of the input tensor `x`. It then generates the positional embeddings using the `torch.arange` function and combines the token, positional, and segment embeddings using element-wise addition. Finally, the method applies layer normalization to the resulting tensor and returns it.\n",
    "\n",
    "Overall, this code defines an embedding layer that can be used in a neural network for natural language processing tasks such as text classification and language modeling. The layer takes in an input sequence and generates embeddings for each token in the sequence, taking into account the position and segment of each token. The layer normalization step helps to stabilize the training process and improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85cd20-b4f1-43f0-99fb-c637a8b2005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "    ):\n",
    "        super(\n",
    "            Embedding,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        # self.tok_embed.shape = (vocab_size, d_model)\n",
    "        self.tok_embed = nn.Embedding(\n",
    "            vocab_size,\n",
    "            d_model,\n",
    "        )  # token embedding\n",
    "        self.pos_embed = nn.Embedding(\n",
    "            max_length,\n",
    "            d_model,\n",
    "        )  # position embedding\n",
    "        self.norm = nn.LayerNorm(d_model)  # layer normalization\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(\n",
    "            seq_len,\n",
    "            dtype=torch.long,\n",
    "        ).cuda()  # 0, 1, 2,... , seq_len-1\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (1, seq_len) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos)  # + self.seg_embed(seg) # (batch_size, seq_len, d_model)\n",
    "        return self.norm(embedding)  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e06dd4",
   "metadata": {},
   "source": [
    "![Alt text](/Users/mcaro/workbench-ai/micro-machines-research/nbs/models/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087856ed",
   "metadata": {},
   "source": [
    "### Attention Mask\n",
    "\n",
    "In the BERT model, the attention mask is used to mask out padding tokens in the input sequence during the attention mechanism. The attention mask is a tensor of the same shape as the attention scores tensor, where each element is either 0 or -inf. The attention scores tensor is computed as the dot product of the query, key, and value tensors, and represents the importance of each token in the input sequence for each output token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c075c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(\n",
    "    seq_q,\n",
    "):\n",
    "    \"\"\"For masking out the padding part of key sequence.\n",
    "    This ensures that the attention mechanism does not attend to padding tokens, which do not contain any useful information.\n",
    "    \"\"\"\n",
    "    (\n",
    "        batch_size,\n",
    "        len_q,\n",
    "    ) = seq_q.size()  # (batch_size, len_q)\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # (batch_size, 1, len_q), True is masked\n",
    "    return pad_attn_mask.expand(\n",
    "        batch_size,\n",
    "        len_q,\n",
    "        len_q,\n",
    "    )  # (batch_size, len_q, len_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa79d9",
   "metadata": {},
   "source": [
    "The attention mask is created using the `get_attn_pad_mask` function, which takes in the input sequence tensor and returns a tensor of the same shape as the attention scores tensor. The attention mask tensor has a value of -inf for padding tokens and a value of 0 for non-padding tokens. This ensures that the attention mechanism does not attend to padding tokens, which do not contain any useful information.\n",
    "\n",
    "In addition to the padding mask, the BERT model also uses a second attention mask called the \"look-ahead\" mask. This mask is used to prevent tokens from attending to future tokens in the input sequence during the attention mechanism. The look-ahead mask is a lower-triangular matrix with -inf values in the upper triangle and 0 values in the lower triangle. This ensures that each token can only attend to previous tokens in the input sequence.\n",
    "\n",
    "The attention mask and look-ahead mask are combined using element-wise addition to create the final attention mask used in the BERT model. The resulting attention mask is a tensor of the same shape as the attention scores tensor, with -inf values for padding tokens and future tokens, and 0 values for non-padding tokens and previous tokens.\n",
    "\n",
    "The `get_attn_pad_mask` function is a utility function used in the implementation of the BERT model. \n",
    "\n",
    "It takes: \n",
    "\n",
    "- `seq_q` in an input tensor of shape `(batch_size, seq_len_q)` and \n",
    "- `seq_k` an input tensor of shape `(batch_size, seq_len_k)` \n",
    "  \n",
    "and returns a tensor of shape `(batch_size, seq_len_q, seq_len_k)` that can be used as a mask for the attention mechanism.\n",
    "\n",
    "\n",
    "The function first creates a tensor `pad_attn_mask` of shape `(batch_size, seq_len_q, seq_len_k)` filled with zeros. It then creates a boolean tensor `pad_mask` of shape `(batch_size, seq_len_k)` that is `True` for padding tokens and `False` for non-padding tokens. This is done by checking if the input tensor `seq_k` is equal to a special padding token.\n",
    "\n",
    "The function then loops over the batch dimension of the input tensor `seq_q` and sets the corresponding row of `pad_attn_mask` to `True` for all columns corresponding to padding tokens in `seq_k`. This is done using the boolean tensor `pad_mask` and the `unsqueeze` method to add a new dimension to the tensor.\n",
    "\n",
    "Finally, the function returns the tensor `pad_attn_mask`. This tensor can be used as a mask for the attention mechanism to ensure that attention is not paid to padding tokens in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6099763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "seq_q = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "        ],\n",
    "        [\n",
    "            4,\n",
    "            5,\n",
    "            0,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "pad_attn_mask = get_attn_pad_mask(seq_q)\n",
    "print(pad_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0728a3d",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244cdb0",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_k,\n",
    "    ):\n",
    "        super(\n",
    "            ScaledDotProductAttention,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        Q,\n",
    "        K,\n",
    "        V,\n",
    "        attn_mask,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Q, K, V: (batch_size, seq_len, d_model)\n",
    "        attn_mask: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(\n",
    "            Q,\n",
    "            K.transpose(\n",
    "                -1,\n",
    "                -2,\n",
    "            ),\n",
    "        ) / np.sqrt(\n",
    "            self.d_k\n",
    "        )  # scores: (batch_size, seq_len, seq_len)\n",
    "        scores.masked_fill_(\n",
    "            attn_mask,\n",
    "            float(\"-inf\"),\n",
    "        )  # Fills elements of self tensor with value where mask is True.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(\n",
    "            attn,\n",
    "            V,\n",
    "        )  # (batch_size, seq_len, d_model)\n",
    "        return (\n",
    "            scores,\n",
    "            context,\n",
    "            attn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "emb = Embedding(\n",
    "    6,\n",
    "    10,\n",
    "    5,\n",
    ")  # max_length, vocab_size, d_model\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            1,\n",
    "            2,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ],\n",
    "        [\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "            0,\n",
    "            0,\n",
    "        ],\n",
    "    ]\n",
    ")  # (batch_size=2, seq_len=10)\n",
    "\n",
    "embeds = emb(inputs)  # INPUTS, SEGMENT\n",
    "\n",
    "AttMask = get_attn_pad_mask(inputs)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "SDPA = ScaledDotProductAttention(d_k=4)\n",
    "\n",
    "(\n",
    "    Score,\n",
    "    Contex,\n",
    "    Attention,\n",
    ") = SDPA(\n",
    "    embeds,\n",
    "    embeds,\n",
    "    embeds,\n",
    "    AttMask,\n",
    ")  # Q, K, V, attn_mask\n",
    "\n",
    "print(\n",
    "    \"Mask: \",\n",
    "    AttMask,\n",
    ")\n",
    "# print('Score: ', Score)\n",
    "print(\n",
    "    \"Attention: \",\n",
    "    Attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e679f2",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a67d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super(\n",
    "            MultiHeadAttention,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.M_Q = nn.Linear(\n",
    "            d_model,\n",
    "            d_k * num_heads,\n",
    "        )\n",
    "        self.M_K = nn.Linear(\n",
    "            d_model,\n",
    "            d_k * num_heads,\n",
    "        )\n",
    "        self.M_V = nn.Linear(\n",
    "            d_model,\n",
    "            d_k * num_heads,\n",
    "        )\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(d_k)\n",
    "        self.output_linear = nn.Linear(\n",
    "            num_heads * d_k,\n",
    "            d_model,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        Q,\n",
    "        K,\n",
    "        V,\n",
    "        attn_mask,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Q, K, V: (batch_size, seq_len, d_model)\n",
    "        attn_mask: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        (\n",
    "            residual,\n",
    "            batch_size,\n",
    "        ) = (\n",
    "            Q,\n",
    "            Q.size(0),\n",
    "        )\n",
    "        q_s = self.M_Q(Q)  # q_s: (batch_size, seq_len, d_k * num_heads)\n",
    "        q_s = q_s.view(\n",
    "            batch_size,\n",
    "            -1,\n",
    "            self.num_heads,\n",
    "            self.d_k,\n",
    "        )  # q_s: (batch_size, seq_len, num_heads, d_k)\n",
    "        q_s = q_s.transpose(\n",
    "            1,\n",
    "            2,\n",
    "        )  # q_s: (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        k_s = self.M_Q(K).view(\n",
    "            batch_size,\n",
    "            -1,\n",
    "            self.num_heads,\n",
    "            self.d_k,\n",
    "        )  # k_s: (batch_size, seq_len, num_heads, d_k)\n",
    "        k_s = k_s.transpose(\n",
    "            1,\n",
    "            2,\n",
    "        )  # k_s: (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        v_s = self.M_Q(V).view(\n",
    "            batch_size,\n",
    "            -1,\n",
    "            self.num_heads,\n",
    "            self.d_k,\n",
    "        )  # v_s: (batch_size, seq_len, num_heads, d_k)\n",
    "        v_s = v_s.transpose(\n",
    "            1,\n",
    "            2,\n",
    "        )  # v_s: (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1)  # attn_mask: (batch_size, 1, seq_len, seq_len)\n",
    "        attn_mask = attn_mask.repeat(\n",
    "            1,\n",
    "            self.num_heads,\n",
    "            1,\n",
    "            1,\n",
    "        )  # attn_mask: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        (\n",
    "            scores,\n",
    "            context,\n",
    "            attn,\n",
    "        ) = self.scaled_dot_product_attention(\n",
    "            q_s,\n",
    "            k_s,\n",
    "            v_s,\n",
    "            attn_mask,\n",
    "        )  # context: (batch_size, num_heads, seq_len, d_k)\n",
    "        context = (\n",
    "            context.transpose(\n",
    "                1,\n",
    "                2,\n",
    "            )\n",
    "            .contiguous()\n",
    "            .view(\n",
    "                batch_size,\n",
    "                -1,\n",
    "                self.num_heads * self.d_k,\n",
    "            )\n",
    "        )  # context: (batch_size, seq_len, num_heads * d_k)\n",
    "\n",
    "        output = self.output_linear(context)  # output: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return (\n",
    "            self.norm(output + residual),\n",
    "            attn,\n",
    "        )  # output: (batch_size, seq_len, d_model), attn: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(\n",
    "            PositionwiseFeedForward,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        self.w_1 = nn.Linear(\n",
    "            d_model,\n",
    "            d_ff,\n",
    "        )\n",
    "        self.w_2 = nn.Linear(\n",
    "            d_ff,\n",
    "            d_model,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=1024,\n",
    "        d_k=64,\n",
    "        num_heads=12,\n",
    "    ):\n",
    "        super(\n",
    "            EncoderLayer,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(\n",
    "            d_k=d_k,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.feed_forward = PositionwiseFeedForward(\n",
    "            d_model,\n",
    "            d_ff=d_model * 4,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        enc_input,\n",
    "        enc_self_attn_mask,\n",
    "    ):\n",
    "        (\n",
    "            enc_output,\n",
    "            attn,\n",
    "        ) = self.enc_self_attn(\n",
    "            Q=enc_input,\n",
    "            K=enc_input,\n",
    "            V=enc_input,\n",
    "            attn_mask=enc_self_attn_mask,\n",
    "        )  # enc_input to same Q,K,V\n",
    "        enc_output = self.feed_forward(enc_output)\n",
    "        return (\n",
    "            enc_output,\n",
    "            attn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69287d2c",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import math\n",
    "\n",
    "\n",
    "def gelu(\n",
    "    x,\n",
    "):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        d_k=64,\n",
    "        d_model=1024,\n",
    "        num_heads=12,\n",
    "        n_layers=6,\n",
    "    ):\n",
    "        super(\n",
    "            BERT,\n",
    "            self,\n",
    "        ).__init__()\n",
    "        self.embedding = Embedding(\n",
    "            max_length,\n",
    "            vocab_size,\n",
    "            d_model,\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    d_model,\n",
    "                    d_k,\n",
    "                    num_heads,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            d_model,\n",
    "            d_model,\n",
    "        )\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(\n",
    "            d_model,\n",
    "            d_model,\n",
    "        )\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "\n",
    "        (\n",
    "            n_vocab,\n",
    "            n_dim,\n",
    "        ) = embed_weight.size()\n",
    "        self.decoder = nn.Linear(\n",
    "            n_dim,\n",
    "            n_vocab,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder.bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "        masked_pos,\n",
    "    ):\n",
    "        output = self.embedding(inputs)  # (batch_size, seq_len, d_model)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(inputs).cuda()  # (batch_size, seq_len, seq_len)\n",
    "        for layer in self.layers:\n",
    "            (\n",
    "                output,\n",
    "                enc_self_attn,\n",
    "            ) = layer(\n",
    "                output,\n",
    "                enc_self_attn_mask,\n",
    "            )  # output: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # The tensor `masked_pos`  indicates the positions of the masked tokens in the input sequence.\n",
    "        # The tensor `output` is the output of the BERT model, which is a tensor of shape `(batch_size, seq_len, d_model)`.\n",
    "        masked_pos = masked_pos[\n",
    "            :,\n",
    "            :,\n",
    "            None,\n",
    "        ].expand(\n",
    "            -1,\n",
    "            -1,\n",
    "            output.size(-1),\n",
    "        )  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        h_masked = torch.gather(\n",
    "            output,\n",
    "            1,\n",
    "            masked_pos,\n",
    "        )  # (batch_size, 1, d_model), gather the masked tokens\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))  # h_masked: (batch_size, 1, d_model) -> Linear(d_model, d_model) -> GELU -> LayerNorm -> (batch_size, 1, d_model)\n",
    "        logist_lm = self.decoder(h_masked) + self.decoder.bias  # (batch_size, 1, n_vocab)\n",
    "        return logist_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b1531",
   "metadata": {},
   "source": [
    "## Get the masked token\n",
    "\n",
    "In the context of the BERT model, `torch.gather(output, 1, masked_pos)` is used to extract the hidden states of the masked tokens from the output of the BERT model.\n",
    "\n",
    "The tensor `output` is the output of the BERT model, which is a tensor of shape `(batch_size, seq_len, d_model)`. The tensor `masked_pos` is a tensor of shape `(batch_size, seq_len, 1)` that indicates the positions of the masked tokens in the input sequence.\n",
    "\n",
    "The `torch.gather` function is used to extract the hidden states of the masked tokens from the tensor `output`. The function takes three arguments: the input tensor, the dimension along which to index the tensor, and the index tensor.\n",
    "\n",
    "In this case, the `output` tensor is indexed along the second dimension (i.e., `dim=1`) using the `masked_pos` tensor as the index tensor. The resulting tensor has shape `(batch_size, 1, d_model)` and contains the hidden states of the masked tokens.\n",
    "\n",
    "Here's an example of how to use `torch.gather`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7e6c",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "In the context of the BERT model, these lines of code define the output layer of the model, which maps the hidden states of the input tokens to a probability distribution over the vocabulary.\n",
    "\n",
    "The tensor `embed_weight` is a pre-trained embedding matrix that maps each token in the vocabulary to a dense vector representation. The tensor has shape `(vocab_size, embedding_size)`, where `vocab_size` is the size of the vocabulary and `embedding_size` is the size of the embedding vector.\n",
    "\n",
    "The code `n_vocab, n_dim = embed_weight.size()` extracts the size of the vocabulary and the size of the embedding vector from the `embed_weight` tensor.\n",
    "\n",
    "The code `self.decoder = nn.Linear(n_dim, n_vocab, bias=False)` defines a linear layer that maps the hidden states of the input tokens to a vector of size `n_vocab`. The linear layer has no bias term (`bias=False`) and uses the pre-trained embedding matrix `embed_weight` as its weight matrix.\n",
    "\n",
    "The code `self.decoder.weight = embed_weight` sets the weight matrix of the linear layer to the pre-trained embedding matrix `embed_weight`.\n",
    "\n",
    "The code `self.decoder.bias = nn.Parameter(torch.zeros(n_vocab))` sets the bias term of the linear layer to a tensor of zeros with shape `(n_vocab,)`. The bias term is represented as a `nn.Parameter` object, which allows it to be learned during training.\n",
    "\n",
    "Together, these lines of code define the output layer of the BERT model, which maps the hidden states of the input tokens to a probability distribution over the vocabulary. The output layer is trained to predict the next token in a sequence during the masked language modeling task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
