{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bb3ff-f742-454d-8080-5e60d8175299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp project.models.bert.infer\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546b148-f35b-431f-a8a3-ef7810c1d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "\n",
    "# | export\n",
    "import os\n",
    "from random import (\n",
    "    shuffle,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from project.models.bert.model import (\n",
    "    BERT,\n",
    ")\n",
    "from project.utils.plot import (\n",
    "    LivePlot,\n",
    ")\n",
    "from torch import (\n",
    "    nn,\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    TensorDataset,\n",
    ")\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "class BERTInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_file,\n",
    "        tokenizer,\n",
    "        vocab_size,\n",
    "        num_heads=6,\n",
    "        n_layers=6,\n",
    "        d_model=1000,\n",
    "        max_length=1024,\n",
    "    ):\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=d_model,\n",
    "            num_hidden_layers=n_layers,\n",
    "            max_length=max_length,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=2048,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=max_length,\n",
    "            initializer_range=0.0001,\n",
    "        )\n",
    "\n",
    "        # Initialize the model\n",
    "        model = BertForMaskedLM(config).cuda()\n",
    "\n",
    "        # Load the state dictionary\n",
    "        state_dict = torch.load(model_file)\n",
    "\n",
    "        # Load this state dictionary into your BERT model\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        sentence,\n",
    "        mask_token=\"[MASK]\",\n",
    "    ):\n",
    "        # Tokenize input sentence and obtain token IDs\n",
    "        inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Move tokens to CUDA\n",
    "        inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
    "        original_input_ids = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        # Use model to get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "\n",
    "        # Get the predicted token IDs for all masked positions\n",
    "        mask_token_indices = torch.where(inputs[\"input_ids\"] == self.tokenizer.mask_token_id)[1]\n",
    "        predicted_token_ids = torch.argmax(\n",
    "            predictions[\n",
    "                0,\n",
    "                mask_token_indices,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Decode and return the predicted tokens\n",
    "        predicted_tokens = [self.tokenizer.decode(token_id.item()) for token_id in predicted_token_ids]\n",
    "\n",
    "        # Replace [MASK] tokens with predicted tokens\n",
    "        original_input_ids[\n",
    "            0,\n",
    "            mask_token_indices,\n",
    "        ] = predicted_token_ids\n",
    "\n",
    "        # Decode the modified tensor to get the complete sentence\n",
    "        decoded_sentence = self.tokenizer.decode(original_input_ids[0])\n",
    "\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "d_model = 1000\n",
    "num_heads = 10\n",
    "gradient_accumulation_steps = 2\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=os.path.join(\n",
    "        \"../.tmp/data\",\n",
    "        \"training\",\n",
    "        \"vocab.txt\",\n",
    "    )\n",
    ")\n",
    "tokenizer.eos_token = \"[SEP]\"\n",
    "tokenizer.bos_token = \"[CLS]\"\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.unknown_token = \"[UNK]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "inference = BERTInference(\n",
    "    \"../.tmp/data/training/bert.pth\",\n",
    "    tokenizer,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_model=d_model,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "inference.inference(\"1. p10 [MASK] p11 [MASK]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
