{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bb3ff-f742-454d-8080-5e60d8175299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp  project.models.bert.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132de7d9",
   "metadata": {},
   "source": [
    "# Create data and Encoders\n",
    "\n",
    "> Just run this Notebook to generate the data in `.tmp`\n",
    "\n",
    "This notebooks creates data to train the BERT in first order logic. It uses `notebooks.utils.prop_logi` module to generates random theorems. Then mask some persentage of tokens.\n",
    "\n",
    "Generated Data:\n",
    "\n",
    "```\n",
    ".tmp\n",
    "└── data\n",
    "    ├── training\n",
    "    │   ├── theorems.json\n",
    "    │   └── vocab.txt\n",
    "    └── validation\n",
    "        ├── theorems.json\n",
    "        └── vocab.txt\n",
    "```\n",
    "\n",
    "The idea is generate a dataset for train and validation, save the datasets on disk `.tmp` and then it is upload it to an s3 buckets to be used by the production pipeline.\n",
    "\n",
    "During research phase we are going to use the `.tmp` directory to shorcut the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import (\n",
    "    randint,\n",
    ")\n",
    "\n",
    "from project.utils.prop_logic import (\n",
    "    Proof,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_random_theorems(\n",
    "    training_size=10,\n",
    "    axioms=10,\n",
    "    teo_steps=100,\n",
    "    max_teorem_size=1000,\n",
    "    numbers=1000,\n",
    "    prepositions=100,\n",
    "):\n",
    "    training_set: list = []\n",
    "    assert numbers >= axioms, \"The number of axioms must be less than the number of numbers\"\n",
    "    while len(training_set) < training_size:\n",
    "        proof = Proof(\n",
    "            numbers=numbers,\n",
    "            prepositions=prepositions,\n",
    "        )\n",
    "\n",
    "        # Generates Axioms\n",
    "        for _ in range(\n",
    "            randint(\n",
    "                1,\n",
    "                axioms,\n",
    "            )\n",
    "        ):\n",
    "            proof.append_axiom(proof.get_random_statement())\n",
    "        proof.generate_steps(\n",
    "            randint(\n",
    "                1,\n",
    "                teo_steps,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        str_proof = str(proof)\n",
    "        if len(str_proof) < max_teorem_size:\n",
    "            training_set.append(str_proof)\n",
    "    return training_set\n",
    "\n",
    "\n",
    "def generates_vocabulary(\n",
    "    numbers=1000,\n",
    "    prepositions=100,\n",
    "):\n",
    "    numbers_list = []\n",
    "    for i in range(\n",
    "        0,\n",
    "        numbers,\n",
    "    ):\n",
    "        numbers_list.append(str(i))\n",
    "\n",
    "    prepos = []\n",
    "    for i in range(\n",
    "        0,\n",
    "        prepositions,\n",
    "    ):\n",
    "        prepos.append(f\"p{i}\")\n",
    "\n",
    "    tokens = [\n",
    "        \"[PAD]\",\n",
    "        \"[UNK]\",\n",
    "        \"[CLS]\",\n",
    "        \"[SEP]\",\n",
    "        \"[MASK]\",\n",
    "    ]\n",
    "    special_word = [\n",
    "        \"nl\",\n",
    "        \"hypothesis\",\n",
    "        \"conclusion\",\n",
    "        \"proof\",\n",
    "    ]\n",
    "    punctuation = [\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"-\",\n",
    "    ]\n",
    "    logic_operators = [\n",
    "        \"|\",\n",
    "        \"&\",\n",
    "        \"!\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"<\",\n",
    "        \">\",\n",
    "    ]\n",
    "    rules = [\n",
    "        \"introduction\",\n",
    "        \"elimination\",\n",
    "        \"axiom\",\n",
    "        \"implication\",\n",
    "        \"conjution\",\n",
    "        \"disjunction\",\n",
    "        \"negation\",\n",
    "        \"biconditional\",\n",
    "    ]\n",
    "    generates_vocabulary = tokens + special_word + punctuation + logic_operators + rules + prepos + numbers_list\n",
    "    return generates_vocabulary\n",
    "\n",
    "\n",
    "ts = generate_random_theorems(\n",
    "    numbers=1000,\n",
    "    prepositions=100,\n",
    ")\n",
    "\n",
    "for teo in ts:\n",
    "    print(\"----------------\")\n",
    "    print(teo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59516d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "RELATIVE_DATA_PATH = \"../.tmp/data\"\n",
    "\n",
    "notebook_path = os.getcwd()\n",
    "data_dir = os.path.join(\n",
    "    notebook_path,\n",
    "    RELATIVE_DATA_PATH,\n",
    ")\n",
    "train_dir = os.path.join(\n",
    "    data_dir,\n",
    "    \"training\",\n",
    ")\n",
    "val_dir = os.path.join(\n",
    "    data_dir,\n",
    "    \"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "\n",
    "\n",
    "def save_to_json(\n",
    "    data,\n",
    "    filename,\n",
    "):\n",
    "    with open(\n",
    "        filename,\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(\n",
    "            data,\n",
    "            f,\n",
    "        )\n",
    "\n",
    "\n",
    "def load_from_json(\n",
    "    filename,\n",
    "):\n",
    "    with open(\n",
    "        filename,\n",
    "        \"r\",\n",
    "    ) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_vocab(\n",
    "    numbers=1000,\n",
    "    prepositions=100,\n",
    "):\n",
    "    vocab = generates_vocabulary(\n",
    "        numbers=numbers,\n",
    "        prepositions=prepositions,\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def crate_training_data(\n",
    "    batch_dir,\n",
    "    batch_size=100,\n",
    "    max_len=1000,\n",
    "    numbers=1000,\n",
    "    prepositions=100,\n",
    "):\n",
    "    print(\"generating_random_theorems()\")\n",
    "    theorems = generate_random_theorems(\n",
    "        batch_size,\n",
    "        max_teorem_size=max_len - 2,\n",
    "        numbers=1000,\n",
    "        prepositions=100,\n",
    "    )  # -2, saving space for CLS and END tokens\n",
    "    print(\"Making theorems\")\n",
    "\n",
    "    file_name = f\"theorems.json\"\n",
    "    file_path = os.path.join(\n",
    "        batch_dir,\n",
    "        file_name,\n",
    "    )\n",
    "    print(f\"Saving to path: {file_path}\")\n",
    "    save_to_json(\n",
    "        theorems,\n",
    "        file_path,\n",
    "    )\n",
    "\n",
    "\n",
    "max_len = 1024\n",
    "batch_size = 100000\n",
    "numbers = 1000\n",
    "prepositions = 100\n",
    "\n",
    "# Create Vocabulary\n",
    "vocab = create_vocab(\n",
    "    numbers=numbers,\n",
    "    prepositions=prepositions,\n",
    ")\n",
    "with open(\n",
    "    os.path.join(\n",
    "        train_dir,\n",
    "        \"vocab.txt\",\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(\"\\n\".join(vocab))\n",
    "with open(\n",
    "    os.path.join(\n",
    "        val_dir,\n",
    "        \"vocab.txt\",\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(\"\\n\".join(vocab))\n",
    "\n",
    "\n",
    "# # create different batches with different max_pred, to ramp up the difficulty\n",
    "crate_training_data(\n",
    "    train_dir,\n",
    "    max_len=max_len,\n",
    "    batch_size=batch_size,\n",
    "    numbers=numbers,\n",
    "    prepositions=prepositions,\n",
    ")\n",
    "\n",
    "# # create validation data\n",
    "crate_training_data(\n",
    "    val_dir,\n",
    "    max_len=max_len,\n",
    "    batch_size=batch_size * 0.20,\n",
    "    numbers=numbers,\n",
    "    prepositions=prepositions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d59143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
